Summary
===============================

llm-rag-assistant is a fully local, retrieval-augmented chatbot powered by llama-cpp-python, designed to answer questions in Spanish using your own Q&A dataset. It uses semantic search via FAISS + multilingual sentence-transformers to retrieve relevant answers, and combines it with a local instruction-tuned LLM (default: Gemma 3 1B Instruct in GGUF format) for contextual response generation.

## üöÄ Features

- üîç Semantic Search with multilingual embeddings (sentence-transformers)
- üß† Local LLM inference without a GPU using optimized GGUF models + llama-cpp-python
- üíª Runs on standard laptops and desktops ‚Äî no CUDA, no GPU, no special hardware required
- üîí No API keys, no cloud dependency ‚Äî fully private and offline
- üóÇÔ∏è Plug-and-play with any Q&A dataset in JSON format

RAG Local - Instrucciones
===============================

Este paquete te permite correr un chatbot de consola con recuperaci√≥n sem√°ntica (RAG) en tu m√°quina, sin necesidad de GPU ni conexi√≥n externa.

Esta versi√≥n funciona en consola. Para usar con interfaz, ver streamit version

Requisitos:
-----------
1. Python 3.9+
2. Install dependencies (recommend `llama-cpp-python >= 0.3.2` for Gemma 3 support):
   pip install "llama-cpp-python>=0.3.2" faiss-cpu sentence-transformers

Probado con python-3.13.5, versiones espec√≠ficas en environment.yml 
    # En mac os, si falla el build probar 
    conda install -c conda-forge llama-cpp-python 
    pip install faiss-cpu sentence-transformers

3. Descargar el modelo GGUF:

   You can work with any of these instruction models:

   - **Gemma 3 1B Instruct (recommended)**
     ```bash
     wget https://huggingface.co/google/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q4_K_M.gguf -O gemma-3-1b-it.Q4_K_M.gguf
     ```
   - **Mistral-7B-Instruct**
     ```bash
     wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf -O mistral-7b-instruct.Q4_K_M.gguf
     ```

   > ‚ÑπÔ∏è Hugging Face may require you to sign in and accept the license before allowing the download. If you hit a 403 error, visit the model page, accept the terms, and rerun the command.

   For security, validate integrity by comparing the `sha256` hash of the downloaded file with the hash published by the model provider.

   ```bash
   sha256 gemma-3-1b-it.Q4_K_M.gguf
   sha256 mistral-7b-instruct.Q4_K_M.gguf
   ```

   - Gemma 3: Gemma license ‚Üí https://ai.google.dev/gemma
   - Mistral 7B: Apache 2.0 ‚Üí https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1

   Recommended steps before downloading Gemma:

   1. Sign in with your Hugging Face account and **accept the license** in the *Files and versions* tab of `google/gemma-3-1b-it-GGUF`.
   2. Log into the CLI: `huggingface-cli login` (or use an access token).
   3. Then run the `wget` or `huggingface-cli download` commands shown above.
   4. Confirm you have `llama-cpp-python >= 0.3.2` installed; older releases throw ‚Äúunknown model architecture: 'gemma3'‚Äù.

   For the `transformers` backend, download the official HF weights and store them in `../models/gemma-3-1b-it-transformers`:

   ```bash
   huggingface-cli download google/gemma-3-1b-it --local-dir ../models/gemma-3-1b-it-transformers --local-dir-use-symlinks False
   ```

   > Requires accepting the model license and being authenticated with `huggingface-cli login`.

4. Construir dataset de preguntas y respuestas

Importante: Guardar en el archivo qa_dataset.json

Debe tener la siguiente estructura (ejemplo)
```json
[
  {
    "pregunta": "¬øCu√°l es el horario de atenci√≥n?",
    "respuesta": "Nuestro horario de atenci√≥n es de lunes a viernes de 9:00 a 18:00 horas y s√°bados de 9:00 a 14:00."
  },
  {
    "pregunta": "¬øC√≥mo puedo contactar con soporte t√©cnico?",
    "respuesta": "Puede contactar con soporte t√©cnico a trav√©s del email soporte@empresa.com, llamando al 900-123-456 o mediante el chat en vivo de nuestra web."
  },
  ...
]
```

5. Armar archivo config.yaml configuraci√≥n del Sistema RAG

Por ejemplo

```bash
models:
  embeddings:
    model_name: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
  generation:
    llama_cpp_model_path: "models/gemma-3-1b-it.Q4_K_M.gguf"
    max_tokens: 256
```

*Nota:* para que funcione con este tipo de dataset de preguntas y respuestas, debe ser un modelo tipo instruct

TODO:
-----
* Agregar configuraci√≥n de temperature

## üß† Arquitectura del sistema

Este es el flujo general del sistema RAG local:

![Arquitectura RAG](assets/diagram-es.svg)

Archivos incluidos:
-------------------
- prepare_embeddings.py ‚Üí genera dataset_index.faiss y qa.json a partir de tu dataset
- chatbot_rag_local.py  ‚Üí ejecuta el chatbot de consola usando llama-cpp
- chatbot_rag_local_transformers.py ‚Üí alternativa que usa Hugging Face transformers
- qa_dataset.json ‚Üí tu base de conocimiento

Pasos:
------
1. Ejecut√°: python prepare_embeddings.py
2. Ejecut√°: python chatbot_rag_local.py
3. Chate√° con tu base de conocimiento usando un bot en espa√±ol :)

### Alternative without llama.cpp (transformers)

To run the chatbot with Hugging Face weights:

1. Install extra deps: `pip install torch transformers accelerate`
2. Download the model repo (`huggingface-cli download ...`, see above) to `../models/gemma-3-1b-it-transformers`
3. Update `config.yaml` if you use a different path/device (see `transformers` section)
4. Run: `python chatbot_rag_local_transformers.py`

> Note: this path needs sufficient hardware (fast CPU or GPU) and enough RAM to load the full model.

### Models available under `../models`

| Model | Strengths | Considerations |
|-------|-----------|----------------|
| `gemma-3-1b-it.Q4_K_M.gguf` | ‚úÖ 1B parameters, Q4_K_M quantization ‚Üí ~2.1‚ÄØGB on disk and fast startup on CPU/MPS.<br>‚úÖ Fine-tuned for Spanish/scientific topics; low hallucination rate.<br>‚úÖ Excellent performance on Apple Silicon (Metal) and AVX2-only CPUs. | ‚ÑπÔ∏è Requires accepting the Gemma license and using `llama-cpp-python` with `gemma3` support (‚â•0.3.2).<br>‚ÑπÔ∏è Smaller size means prompts may need to be more explicit for very long answers. |
| `mistral-7b-instruct.Q4_K_M.gguf` | ‚úÖ 7B parameters, very robust with generic prompts.<br>‚úÖ Widely battle-tested in the community. | ‚ö†Ô∏è ~4.1‚ÄØGB on disk ‚Üí slower load and generation on pure CPU.<br>‚ö†Ô∏è Higher RAM usage (~7‚Äì8‚ÄØGB with long contexts). |
| `qwen2.5-1.5b-instruct-q2_k.gguf` (optional) | ‚úÖ Ultra-light (<1‚ÄØGB), ideal for constrained hardware.<br>‚úÖ Solid multilingual coverage. | ‚ö†Ô∏è Aggressive Q2 quantization and lower semantic fidelity.<br>‚ö†Ô∏è Needs carefully structured prompts for detailed outputs. |

**Recommendation**: Gemma 3 1B Instruct hits the sweet spot for this project‚Äîfast on Apple‚ÄØM-series/Mac and CPU-only PCs, accurate in Spanish, and modest on resources. Keep Mistral as a backup if you need longer answers and have extra RAM.

## üìä Evaluaci√≥n y M√©tricas de Calidad

### Scripts de Evaluaci√≥n Disponibles

#### üìà Evaluaci√≥n con BERTScore
Para evaluar la calidad sem√°ntica de las respuestas del sistema RAG:

```bash
# 1. Generar respuestas
python model_evaluation.py

# 2. Solo calcular BERTScore de respuestas ya generadas
python model_evaluation.py bertscore
```

**Salida**: 
- `evaluation_results.json` - Pares pregunta-respuesta generados
- `bertscore_results.json` - M√©tricas BERTScore detalladas

#### üìä Calcular M√©tricas desde Archivo Existente
Si ya tienes un archivo con respuestas generadas:

```bash
# Calcular todas las m√©tricas desde evaluation_results.json
python calculate_metrics_from_json.py

# O desde cualquier archivo JSON espec√≠fico
python calculate_metrics_from_json.py mi_archivo_evaluacion.json
```

### üìã Dependencias para Evaluaci√≥n

```bash
# Instalar m√©tricas de evaluaci√≥n
pip install bert-score rouge-score nltk scikit-learn
```

### üéØ Interpretaci√≥n de M√©tricas

#### BERTScore (Similitud Sem√°ntica)
- **F1 > 0.85**: ‚≠ê Excelente - Listo para producci√≥n
- **F1 0.70-0.85**: ‚úÖ Bueno - Uso acad√©mico confiable  
- **F1 0.50-0.70**: ‚ö†Ô∏è B√°sico - Requiere mejoras
- **F1 < 0.50**: ‚ùå Problem√°tico - Revisar sistema

#### ROUGE (N-gramas)
- **ROUGE-1**: Coincidencia de palabras individuales
- **ROUGE-2**: Coincidencia de pares de palabras  
- **ROUGE-L**: Secuencias largas coincidentes

#### BLEU (Precisi√≥n)
- **BLEU-4**: Precisi√≥n de 4-gramas (m√°s estricto)
- Valores t√≠picos: 0.2-0.6 para generaci√≥n natural

### üîß Configuraci√≥n de Evaluaci√≥n

Para personalizar la evaluaci√≥n, edita los par√°metros en los scripts:

```python
# N√∫mero de muestras a evaluar
n_samples = 15  # Cambiar seg√∫n necesidad

# Seed para reproducibilidad
random.seed(42)

# Idioma para BERTScore
lang="es"  # Espa√±ol por defecto
```

### üìä Ejemplo de Salida

```
üìä BERTScore (Similitud Sem√°ntica):
   Precision: 0.7724 ¬± 0.0879
   Recall:    0.8905 ¬± 0.0591  
   F1-Score:  0.8265 ¬± 0.0732
   Calidad:   ‚≠ê MUY BUENO

üìù ROUGE (N-gramas y Secuencias):
   ROUGE-1:   0.5064 ¬± 0.2007
   ROUGE-2:   0.4026 ¬± 0.2220
   ROUGE-L:   0.4760 ¬± 0.2138
```

Requisitos:
-----------
- 8GB RAM m√≠nimo (16GB recomendado)
- ~5GB de espacio para los modelos
