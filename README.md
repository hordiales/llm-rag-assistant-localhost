Summary
===============================

llm-rag-assistant is a fully local, retrieval-augmented chatbot powered by llama-cpp-python. It answers questions in Spanish using your own Q&A dataset: FAISS + multilingual sentence-transformers retrieve relevant context, and a local instruction-tuned LLM (default: Gemma 3 1B Instruct, GGUF) generates the response.

> Looking for the Spanish version? See `README_es.md`.

## ğŸš€ Features

- ğŸ” Semantic search with multilingual sentence-transformers
- ğŸ§  Local LLM inference with llama-cpp-python (CPU friendly GGUF models)
- ğŸ’» Runs on standard laptops/desktops â€” no GPU or CUDA required
- ğŸ”’ 100% offline, no API keys or external services
- ğŸ—‚ï¸ Works with any JSON Q&A dataset

Quick Start (RAG Console Bot)
===============================

This repository ships a console-based RAG chatbot that runs entirely offline.

Requirements
------------
1. Python 3.9+
2. Install dependencies (recommend `llama-cpp-python >= 0.3.2` for Gemma 3 support):
   ```bash
   pip install "llama-cpp-python>=0.3.2" faiss-cpu sentence-transformers
   ```
   On macOS you can fall back to conda if compilation fails:
   ```bash
   conda install -c conda-forge llama-cpp-python
   pip install faiss-cpu sentence-transformers
   ```
3. Download a GGUF model and place it under `../models/`:
   - **Gemma 3 1B Instruct (recommended)**
     ```bash
     wget https://huggingface.co/google/gemma-3-1b-it-GGUF/resolve/main/gemma-3-1b-it-Q4_K_M.gguf \
       -O gemma-3-1b-it.Q4_K_M.gguf
     ```
   - **Mistral-7B-Instruct**
     ```bash
     wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf \
       -O mistral-7b-instruct.Q4_K_M.gguf
     ```

   > â„¹ï¸ Hugging Face may require you to sign in and accept the license before downloading. If you hit a 403 error, open the model page, accept the terms, and rerun the command.

   Always verify file integrity by comparing the `sha256` hash against the value published by the model provider:
   ```bash
   sha256 gemma-3-1b-it.Q4_K_M.gguf
   sha256 mistral-7b-instruct.Q4_K_M.gguf
   ```

   - Gemma 3: Gemma license â†’ https://ai.google.dev/gemma
   - Mistral 7B: Apache 2.0 â†’ https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1

   Recommended steps before downloading Gemma:
   1. Sign in with your Hugging Face account and **accept the license** in the *Files and versions* tab of `google/gemma-3-1b-it-GGUF`.
   2. Log into the CLI: `huggingface-cli login` (or use an access token).
   3. Run the `wget`/`huggingface-cli download` commands above.
   4. Confirm `llama-cpp-python >= 0.3.2`; older releases throw â€œunknown model architecture: 'gemma3'â€.

   Transformers backend (optional):
   ```bash
   huggingface-cli download google/gemma-3-1b-it \
     --local-dir ../models/gemma-3-1b-it-transformers \
     --local-dir-use-symlinks False
   ```
   > Requires license acceptance and `huggingface-cli login`.

4. Build your Q&A dataset in `qa_dataset.json`:
   ```json
   [
     {
       "pregunta": "Â¿CuÃ¡l es el horario de atenciÃ³n?",
       "respuesta": "Nuestro horario es de lunes a viernes de 9 a 18 y sÃ¡bados de 9 a 14."
     },
     {
       "pregunta": "Â¿CÃ³mo puedo contactar con soporte tÃ©cnico?",
       "respuesta": "Puedes escribir a soporte@empresa.com o llamar al 900-123-456."
     }
   ]
   ```
5. Configure `config.yaml`:
   ```yaml
   models:
     embeddings:
       model_name: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
     generation:
       llama_cpp_model_path: "../models/gemma-3-1b-it.Q4_K_M.gguf"
       max_tokens: 256
   ```

Project Structure
-----------------
- `prepare_embeddings.py` â†’ builds `dataset_index.faiss` and `qa.json`
- `chatbot_rag_local.py` â†’ console chatbot using llama-cpp
- `chatbot_rag_local_transformers.py` â†’ transformers-based alternative
- `qa_dataset.json` â†’ user knowledge base (input)

Usage
-----
1. `python prepare_embeddings.py`
2. `python chatbot_rag_local.py`
3. Chat with your knowledge base in Spanish :)

Alternative without llama.cpp (transformers)
-------------------------------------------
1. `pip install torch transformers accelerate`
2. Download Hugging Face weights to `../models/gemma-3-1b-it-transformers`
3. Update `config.yaml` â†’ `transformers` section (path/device/dtype)
4. `python chatbot_rag_local_transformers.py`

> Needs ample RAM (12â€¯GB+ recommended) or a GPU for smooth inference.

Models available under `../models`
----------------------------------
| Model | Strengths | Considerations |
|-------|-----------|----------------|
| `gemma-3-1b-it.Q4_K_M.gguf` | âœ… 1B parameters, Q4_K_M quantization (~2.1â€¯GB). Fast startup on CPU/MPS.<br>âœ… Tuned for Spanish/science; low hallucination rate.<br>âœ… Works great on Apple Silicon and AVX2-only CPUs. | â„¹ï¸ Must accept Gemma license and use `llama-cpp-python` â‰¥0.3.2.<br>â„¹ï¸ Smaller model may need more explicit prompts for long answers. |
| `mistral-7b-instruct.Q4_K_M.gguf` | âœ… 7B parameters, robust with generic prompts.<br>âœ… Widely battle-tested. | âš ï¸ ~4.1â€¯GB on disk, slower on CPU.<br>âš ï¸ Higher RAM usage (~7â€“8â€¯GB with long contexts). |
| `qwen2.5-1.5b-instruct-q2_k.gguf` | âœ… Ultra-light (<1â€¯GB), ideal for tight hardware budgets.<br>âœ… Good multilingual coverage. | âš ï¸ Aggressive Q2 quantization â†’ lower fidelity.<br>âš ï¸ Needs carefully structured prompts. |

**Recommendation**: Gemma 3 1B Instruct offers the best balance for this projectâ€”fast, accurate in Spanish, and resource-friendly. Keep Mistral as a backup if you need longer answers and have extra RAM.

Evaluation & Metrics
====================

Available scripts
-----------------
- `model_evaluation.py` â†’ generates answers (default) or runs BERTScore (`python model_evaluation.py bertscore`)
- `calculate_metrics_from_json.py` â†’ recomputes BERTScore, ROUGE, BLEU, cosine similarity from an existing JSON
- `real_rag_evaluation.py` â†’ end-to-end evaluation of the live RAG pipeline

Sample workflow
---------------
```bash
# 1. Generate answers
python model_evaluation.py

# 2. Compute BERTScore on existing results
python model_evaluation.py bertscore
```

Outputs:
- `evaluation_results.json` â€“ question/ground-truth/generated triples
- `bertscore_results.json` â€“ BERTScore stats and per-sample metrics

Metric interpretation
---------------------
- **BERTScore F1**
  - > 0.85 â†’ Excellent
  - 0.70â€“0.85 â†’ Good
  - 0.50â€“0.70 â†’ Needs improvement
  - < 0.50 â†’ Problematic
- **ROUGE-1/2/L** â†’ Unigram/bigram/longest-sequence overlap
- **BLEU-4** â†’ 4-gram precision (typical range 0.2â€“0.6 for natural text)

Configuration knobs (in the scripts)
------------------------------------
```python
n_samples = 15        # Number of Q&A pairs to evaluate
random.seed(42)       # Reproducibility
lang = "es"           # Language for BERTScore
```

Example output
--------------
```
ğŸ“Š BERTScore (semantic similarity):
   Precision: 0.7724 Â± 0.0879
   Recall:    0.8905 Â± 0.0591
   F1-Score:  0.8265 Â± 0.0732

ğŸ“ ROUGE:
   ROUGE-1:   0.5064 Â± 0.2007
   ROUGE-2:   0.4026 Â± 0.2220
   ROUGE-L:   0.4760 Â± 0.2138
```

Hardware recommendations
------------------------
- Minimum 8â€¯GB RAM (16â€¯GB preferred)
- ~5â€¯GB free disk space for models and indexes
