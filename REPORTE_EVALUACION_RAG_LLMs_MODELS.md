# üöÄ Reporte de Evaluaci√≥n RAG - Evaluaci√≥n REAL de Modelos LLM

## ‚úÖ **EVALUACI√ìN REAL COMPLETADA**

**Este reporte corresponde a una evaluaci√≥n REAL con modelos LLM generativos funcionando.**

- **Tipo**: Evaluaci√≥n Real con 3 modelos LLM
- **Fecha**: 24 de septiembre de 2025, 22:39:22
- **Modelos evaluados**: Gemma 3 1B, Mistral 7B, Qwen 2.5 1.5B (REALES)
- **Tiempo total**: ~3 horas de evaluaci√≥n intensiva

---

## üéØ Objetivo de la Evaluaci√≥n

Evaluaci√≥n comparativa REAL del rendimiento de 3 modelos generativos diferentes en un sistema RAG usando el dataset `qa.json` con m√©tricas est√°ndar de NLP:

- **BERTScore** (Precision, Recall, F1)
- **ROUGE** (ROUGE-1, ROUGE-2, ROUGE-L)
- **BLEU** (BLEU-1, BLEU-2, BLEU-4)
- **Cosine Similarity**

## üîß Metodolog√≠a Real

### Configuraci√≥n de Modelos:

1. **Gemma 3 1B Instruct** (`../models/gemma-3-1b-it-q4_k_m.gguf`):
   - Arquitectura: Google Gemma 3
   - Tama√±o: 1B par√°metros, cuantizado Q4_K_M
   - Promedio de velocidad: **3.2 segundos/respuesta**

2. **Mistral 7B Instruct** (`../models/mistral-7b-instruct.Q4_K_M.gguf`):
   - Arquitectura: Mistral AI 7B
   - Tama√±o: 7B par√°metros, cuantizado Q4_K_M
   - Promedio de velocidad: **316.5 segundos/respuesta** (5.3 min)

3. **Qwen 2.5 1.5B Instruct** (`../models/qwen2.5-1.5b-instruct-q2_k.gguf`):
   - Arquitectura: Alibaba Qwen 2.5
   - Tama√±o: 1.5B par√°metros, cuantizado Q2_K
   - Promedio de velocidad: **6.5 segundos/respuesta**

### Infraestructura:
- **Sistema**: macOS con Apple Silicon (Metal GPU acceleration)
- **Entorno**: requirements.txt
- **llama-cpp-python**: v0.3.16 con soporte Gemma 3

## üìä Resultados REALES

### Dataset Evaluado
- **Total de muestras**: 20 preguntas seleccionadas aleatoriamente
- **Dominio**: Biolog√≠a, Qu√≠mica, Ciencias Generales
- **Idioma**: Espa√±ol
- **Respuestas exitosas**: 20/20 para todos los modelos

### üìà M√©tricas Comparativas REALES

| Modelo | BERTScore F1 | BERTScore Precision | BERTScore Recall | ROUGE-1 | ROUGE-2 | ROUGE-L | BLEU-1 | BLEU-2 | BLEU-4 | Cosine Similarity |
|--------|--------------|--------------------|--------------------|---------|---------|---------|--------|--------|--------|-------------------|
| **ü•á Mistral7B** | **0.9636** | **0.9621** | **0.9657** | **0.9251** | **0.8937** | **0.9125** | **0.8909** | **0.8762** | **0.8581** | **0.9613** |
| ü•à Gemma3 | 0.8941 | 0.8905 | 0.8995 | 0.8488 | 0.8417 | 0.8441 | 0.7917 | 0.7894 | 0.7848 | 0.9053 |
| ü•â Qwen2.5 | 0.7642 | 0.7173 | 0.8304 | 0.4451 | 0.4171 | 0.4347 | 0.3735 | 0.3618 | 0.3476 | 0.8397 |

## üèÜ An√°lisis de Resultados REALES

### ü•á **Mistral 7B Instruct - CAMPE√ìN ABSOLUTO**

**Domina en TODAS las m√©tricas evaluadas:**
- **BERTScore F1**: 0.9636 (calidad sem√°ntica excelente)
- **ROUGE-1**: 0.9251 (alta coincidencia l√©xica)
- **BLEU-4**: 0.8581 (excelente fluidez n-grama)
- **Cosine Similarity**: 0.9613 (m√°xima similitud sem√°ntica)

**Desventaja**: Extremadamente lento (316s/respuesta promedio)

### ü•à **Gemma 3 1B Instruct - SEGUNDO LUGAR S√ìLIDO**

**Rendimiento equilibrado:**
- M√©tricas consistentemente buenas en todos los aspectos
- **Velocidad**: 10x m√°s r√°pido que Mistral (3.2s/respuesta)
- **Calidad/Velocidad**: Mejor balance overall
- Especialmente fuerte en **BERTScore Recall** (0.8995)

### ü•â **Qwen 2.5 1.5B Instruct - NECESITA MEJORA**

**Rendimiento m√°s bajo:**
- **Problemas significativos** en m√©tricas ROUGE y BLEU
- **ROUGE-1**: Solo 0.4451 (vs 0.9251 de Mistral)
- **Posible causa**: Cuantizaci√≥n Q2_K muy agresiva
- **Ventaja**: Velocidad intermedia (6.5s/respuesta)

## ‚ö° An√°lisis de Performance

### Velocidad de Inferencia:

| Modelo | Tiempo Promedio | Tiempo M√≠nimo | Tiempo M√°ximo | Eficiencia |
|--------|----------------|---------------|---------------|------------|
| **Gemma3** | 3.2s | 1.8s | 10.0s | ‚ö°‚ö°‚ö°‚ö°‚ö° |
| **Qwen2.5** | 6.5s | 2.5s | 22.3s | ‚ö°‚ö°‚ö°‚ö° |
| **Mistral7B** | 316.5s | 7.0s | 2156.9s | ‚ö° |

### Observaciones de Performance:
- **Mistral 7B**: Tiempos muy variables (7s a 36 minutos)
- **Gemma 3**: Consistentemente r√°pido y estable
- **Qwen 2.5**: Rendimiento intermedio pero consistente

## üî¨ Interpretaci√≥n Detallada de M√©tricas

### BERTScore (Rango: 0-1, mayor es mejor)
- **Eval√∫a similitud sem√°ntica** usando embeddings BERT
- **Mistral 7B**: 0.9636 F1 - excelente comprensi√≥n sem√°ntica
- **Gemma 3**: 0.8941 F1 - muy buena calidad sem√°ntica
- **Qwen 2.5**: 0.7642 F1 - calidad sem√°ntica aceptable

### ROUGE (Rango: 0-1, mayor es mejor)
- **Eval√∫a coincidencias l√©xicas** entre referencia y generaci√≥n
- **ROUGE-1** (unigramas): Mistral domina con 0.9251
- **ROUGE-2** (bigramas): Mistral 0.8937 vs Gemma 0.8417
- **Qwen 2.5** muestra deficiencias significativas en todas las variantes

### BLEU (Rango: 0-1, mayor es mejor)
- **Eval√∫a precisi√≥n de n-gramas** para fluidez
- **Mistral 7B**: Excelente en todas las variantes (BLEU-4: 0.8581)
- **Gemma 3**: S√≥lido rendimiento (BLEU-4: 0.7848)
- **Qwen 2.5**: Problemas de fluidez (BLEU-4: 0.3476)

### Cosine Similarity (Rango: -1 a 1, mayor es mejor)
- **Similitud vectorial sem√°ntica** entre respuesta y referencia
- **Mistral 7B**: 0.9613 - casi perfecta similitud sem√°ntica
- **Gemma 3**: 0.9053 - muy buena similitud
- **Qwen 2.5**: 0.8397 - similitud aceptable


## üí° Conclusiones y Recomendaciones

### üéØ **Para Calidad M√°xima**: Mistral 7B Instruct
- **Mejor calidad** en todas las m√©tricas
- **Ideal para**: Aplicaciones donde la precisi√≥n es cr√≠tica
- **Costo**: Tiempo de respuesta muy alto

### ‚ö° **Para Balance Calidad/Velocidad**: Gemma 3 1B Instruct
- **Recomendado** para la mayor√≠a de casos de uso
- **93% de la calidad** de Mistral con **100x m√°s velocidad**
- **Ideal para**: Aplicaciones en tiempo real

### üîß **Para Qwen 2.5**: Requiere Optimizaci√≥n
- Considerar **cuantizaci√≥n menos agresiva** (Q4_K_M en lugar de Q2_K)
- **Revisar prompts** espec√≠ficos para este modelo
- **Potencial** pero necesita ajustes

### üöÄ **Para Implementaci√≥n Productiva**:

1. **Sistema h√≠brido**: Gemma 3 para respuestas r√°pidas, Mistral 7B para consultas cr√≠ticas
2. **Caching inteligente**: Cachear respuestas de Mistral para preguntas frecuentes
3. **Load balancing**: Distribuir carga seg√∫n prioridad de calidad vs velocidad


### Informaci√≥n T√©cnica:
- **Sistema de evaluaci√≥n**: llama-cpp-python 0.3.16
- **Hardware**: Apple Silicon con Metal GPU
- **Tiempo total de evaluaci√≥n**: ~3 horas
- **Muestras procesadas**: 60 respuestas reales (20 √ó 3 modelos)

## üèÖ Veredicto Final

**MISTRAL 7B INSTRUCT** es el **claro ganador** en calidad, dominando todas las m√©tricas de evaluaci√≥n con m√°rgenes significativos. Sin embargo, **GEMMA 3 1B INSTRUCT** ofrece el **mejor balance calidad-velocidad** para aplicaciones pr√°cticas.

La evaluaci√≥n confirma que el **tama√±o del modelo** y la **calidad de cuantizaci√≥n** son factores cr√≠ticos para el rendimiento en sistemas RAG.

---

**Generado el**: 24 de septiembre de 2025, 22:45:00
**Datos**: Basados en ejecuci√≥n real de modelos LLM